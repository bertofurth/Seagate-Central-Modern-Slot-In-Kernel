diff -rup linux-5.15.78-orig/mm/kasan/init.c linux-5.15.78/mm/kasan/init.c
--- linux-5.15.78-orig/mm/kasan/init.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/kasan/init.c	2022-11-16 10:13:35.846580669 +1100
@@ -343,8 +343,11 @@ static void kasan_remove_pte_table(pte_t
 				unsigned long end)
 {
 	unsigned long next;
-
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr < end; addr = next, pte += PTE_STEP) {
+#else
 	for (; addr < end; addr = next, pte++) {
+#endif
 		next = (addr + PAGE_SIZE) & PAGE_MASK;
 		if (next > end)
 			next = end;
diff -rup linux-5.15.78-orig/mm/madvise.c linux-5.15.78/mm/madvise.c
--- linux-5.15.78-orig/mm/madvise.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/madvise.c	2022-11-16 10:13:35.846580669 +1100
@@ -396,7 +396,11 @@ regular_page:
 	orig_pte = pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 	flush_tlb_batched_pending(mm);
 	arch_enter_lazy_mmu_mode();
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr < end; pte += PTE_STEP, addr += PAGE_SIZE) {
+#else
 	for (; addr < end; pte++, addr += PAGE_SIZE) {
+#endif
 		ptent = *pte;
 
 		if (pte_none(ptent))
@@ -596,7 +600,11 @@ static int madvise_free_pte_range(pmd_t
 	orig_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
 	flush_tlb_batched_pending(mm);
 	arch_enter_lazy_mmu_mode();
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr != end; pte += PTE_STEP, addr += PAGE_SIZE) {
+#else
 	for (; addr != end; pte++, addr += PAGE_SIZE) {
+#endif
 		ptent = *pte;
 
 		if (pte_none(ptent))
Only in linux-5.15.78/mm/: madvise.c.orig
diff -rup linux-5.15.78-orig/mm/memcontrol.c linux-5.15.78/mm/memcontrol.c
--- linux-5.15.78-orig/mm/memcontrol.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/memcontrol.c	2022-11-16 10:13:35.846580669 +1100
@@ -5851,7 +5851,11 @@ static int mem_cgroup_count_precharge_pt
 	if (pmd_trans_unstable(pmd))
 		return 0;
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr != end; pte += PTE_STEP, addr += PAGE_SIZE)
+#else
 	for (; addr != end; pte++, addr += PAGE_SIZE)
+#endif
 		if (get_mctgt_type(vma, addr, *pte, NULL))
 			mc.precharge++;	/* increment precharge temporarily */
 	pte_unmap_unlock(pte - 1, ptl);
@@ -6071,10 +6075,14 @@ static int mem_cgroup_move_charge_pte_ra
 retry:
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
 	for (; addr != end; addr += PAGE_SIZE) {
-		pte_t ptent = *(pte++);
+		pte_t ptent = *pte;
 		bool device = false;
 		swp_entry_t ent;
-
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+		pte += PTE_STEP;
+#else
+		pte++;
+#endif
 		if (!mc.precharge)
 			break;
 
diff -rup linux-5.15.78-orig/mm/memory.c linux-5.15.78/mm/memory.c
--- linux-5.15.78-orig/mm/memory.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/memory.c	2022-11-16 10:13:35.846580669 +1100
@@ -1086,8 +1086,11 @@ again:
 			prealloc = NULL;
 		}
 		progress += 8;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (dst_pte += PTE_STEP, src_pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
-
+#endif
 	arch_leave_lazy_mmu_mode();
 	spin_unlock(src_ptl);
 	pte_unmap(orig_src_pte);
@@ -1424,8 +1427,11 @@ again:
 		if (unlikely(!free_swap_and_cache(entry)))
 			print_bad_pte(vma, addr, ptent, NULL);
 		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
-
+#endif
 	add_mm_rss_vec(mm, rss);
 	arch_leave_lazy_mmu_mode();
 
@@ -1827,14 +1833,20 @@ static int insert_pages(struct vm_area_s
 	unsigned long pages_to_write_in_pmd;
 	int ret;
 more:
+
 	ret = -EFAULT;
 	pmd = walk_to_pmd(mm, addr);
 	if (!pmd)
 		goto out;
 
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	pages_to_write_in_pmd = min_t(unsigned long,
+		remaining_pages_total, PTRS_PER_PTE_REAL - pte_index(addr));
+#else
 	pages_to_write_in_pmd = min_t(unsigned long,
 		remaining_pages_total, PTRS_PER_PTE - pte_index(addr));
-
+#endif
+	
 	/* Allocate the PTE if necessary; takes PMD lock once only. */
 	ret = -ENOMEM;
 	if (pte_alloc(mm, pmd))
@@ -1845,7 +1857,11 @@ more:
 		const int batch_size = min_t(int, pages_to_write_in_pmd, 8);
 
 		start_pte = pte_offset_map_lock(mm, pmd, addr, &pte_lock);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+		for (pte = start_pte; pte_idx < batch_size; pte += PTE_STEP, ++pte_idx) {
+#else
 		for (pte = start_pte; pte_idx < batch_size; ++pte, ++pte_idx) {
+#endif
 			int err = insert_page_in_batch_locked(mm, pte,
 				addr, pages[curr_page_idx], prot);
 			if (unlikely(err)) {
@@ -1890,7 +1906,6 @@ int vm_insert_pages(struct vm_area_struc
 {
 #ifdef pte_index
 	const unsigned long end_addr = addr + (*num * PAGE_SIZE) - 1;
-
 	if (addr < vma->vm_start || end_addr >= vma->vm_end)
 		return -EFAULT;
 	if (!(vma->vm_flags & VM_MIXEDMAP)) {
@@ -2307,7 +2322,11 @@ static int remap_pte_range(struct mm_str
 		}
 		set_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));
 		pfn++;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	arch_leave_lazy_mmu_mode();
 	pte_unmap_unlock(mapped_pte, ptl);
 	return err;
@@ -2542,7 +2561,11 @@ static int apply_to_pte_range(struct mm_
 	if (fn) {
 		do {
 			if (create || !pte_none(*pte)) {
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+			        err = fn(pte += PTE_STEP, addr, data);
+#else
 				err = fn(pte++, addr, data);
+#endif
 				if (err)
 					break;
 			}
diff -rup linux-5.15.78-orig/mm/mempolicy.c linux-5.15.78/mm/mempolicy.c
--- linux-5.15.78-orig/mm/mempolicy.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/mempolicy.c	2022-11-16 10:13:35.846580669 +1100
@@ -521,7 +521,11 @@ static int queue_pages_pte_range(pmd_t *
 		return 0;
 
 	mapped_pte = pte = pte_offset_map_lock(walk->mm, pmd, addr, &ptl);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr != end; pte += PTE_STEP, addr += PAGE_SIZE) {
+#else
 	for (; addr != end; pte++, addr += PAGE_SIZE) {
+#endif
 		if (!pte_present(*pte))
 			continue;
 		page = vm_normal_page(vma, addr, *pte);
diff -rup linux-5.15.78-orig/mm/mlock.c linux-5.15.78/mm/mlock.c
--- linux-5.15.78-orig/mm/mlock.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/mlock.c	2022-11-16 10:13:35.854580753 +1100
@@ -370,7 +370,11 @@ static unsigned long __munlock_pagevec_f
 	start += PAGE_SIZE;
 	while (start < end) {
 		struct page *page = NULL;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+		pte += PTE_STEP;
+#else
 		pte++;
+#endif
 		if (pte_present(*pte))
 			page = vm_normal_page(vma, start, *pte);
 		/*
diff -rup linux-5.15.78-orig/mm/mprotect.c linux-5.15.78/mm/mprotect.c
--- linux-5.15.78-orig/mm/mprotect.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/mprotect.c	2022-11-16 10:13:35.854580753 +1100
@@ -187,7 +187,11 @@ static unsigned long change_pte_range(st
 				pages++;
 			}
 		}
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	arch_leave_lazy_mmu_mode();
 	pte_unmap_unlock(pte - 1, ptl);
 
diff -rup linux-5.15.78-orig/mm/mremap.c linux-5.15.78/mm/mremap.c
--- linux-5.15.78-orig/mm/mremap.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/mremap.c	2022-11-16 10:13:35.854580753 +1100
@@ -175,8 +175,13 @@ static void move_ptes(struct vm_area_str
 	flush_tlb_batched_pending(vma->vm_mm);
 	arch_enter_lazy_mmu_mode();
 
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; old_addr < old_end; old_pte += PTE_STEP, old_addr += PAGE_SIZE,
+				   new_pte += PTE_STEP, new_addr += PAGE_SIZE) {
+#else	
 	for (; old_addr < old_end; old_pte++, old_addr += PAGE_SIZE,
 				   new_pte++, new_addr += PAGE_SIZE) {
+#endif
 		if (pte_none(*old_pte))
 			continue;
 
diff -rup linux-5.15.78-orig/mm/page_vma_mapped.c linux-5.15.78/mm/page_vma_mapped.c
--- linux-5.15.78-orig/mm/page_vma_mapped.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/page_vma_mapped.c	2022-11-16 10:13:35.854580753 +1100
@@ -278,7 +278,11 @@ next_pte:
 				pvmw->pte = NULL;
 				goto restart;
 			}
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+			pvmw->pte += PTE_STEP;
+#else
 			pvmw->pte++;
+#endif
 			if ((pvmw->flags & PVMW_SYNC) && !pvmw->ptl) {
 				pvmw->ptl = pte_lockptr(mm, pvmw->pmd);
 				spin_lock(pvmw->ptl);
diff -rup linux-5.15.78-orig/mm/pagewalk.c linux-5.15.78/mm/pagewalk.c
--- linux-5.15.78-orig/mm/pagewalk.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/pagewalk.c	2022-11-16 10:13:35.854580753 +1100
@@ -33,7 +33,11 @@ static int walk_pte_range_inner(pte_t *p
 		if (addr >= end - PAGE_SIZE)
 			break;
 		addr += PAGE_SIZE;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+		pte += PTE_STEP;
+#else
 		pte++;
+#endif
 	}
 	return err;
 }
diff -rup linux-5.15.78-orig/mm/swapfile.c linux-5.15.78/mm/swapfile.c
--- linux-5.15.78-orig/mm/swapfile.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/swapfile.c	2022-11-16 10:13:35.854580753 +1100
@@ -2000,7 +2000,11 @@ static int unuse_pte_range(struct vm_are
 		}
 try_next:
 		pte = pte_offset_map(pmd, addr);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	pte_unmap(pte - 1);
 
 	ret = 0;
diff -rup linux-5.15.78-orig/mm/swap_state.c linux-5.15.78/mm/swap_state.c
--- linux-5.15.78-orig/mm/swap_state.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/swap_state.c	2022-11-16 10:13:35.854580753 +1100
@@ -765,9 +765,17 @@ static void swap_ra_info(struct vm_fault
 	ra_info->ptes = pte;
 #else
 	tpte = ra_info->ptes;
-	for (pfn = start; pfn != end; pfn++)
+	for (pfn = start; pfn != end; pfn++) {
+
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+		*tpte = *pte;
+		tpte += PTE_STEP;
+		pte += PTE_STEP;
+#else
 		*tpte++ = *pte++;
 #endif
+	}
+#endif
 	pte_unmap(orig_pte);
 }
 
@@ -804,8 +812,13 @@ static struct page *swap_vma_readahead(s
 		goto skip;
 
 	blk_start_plug(&plug);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (i = 0, pte = ra_info.ptes; i < ra_info.nr_pte;
+	     i++, pte += PTE_STEP) {
+#else
 	for (i = 0, pte = ra_info.ptes; i < ra_info.nr_pte;
 	     i++, pte++) {
+#endif
 		pentry = *pte;
 		if (pte_none(pentry))
 			continue;
diff -rup linux-5.15.78-orig/mm/vmalloc.c linux-5.15.78/mm/vmalloc.c
--- linux-5.15.78-orig/mm/vmalloc.c	2022-11-11 04:15:43.000000000 +1100
+++ linux-5.15.78/mm/vmalloc.c	2022-11-16 10:13:35.854580753 +1100
@@ -125,7 +125,11 @@ static int vmap_pte_range(pmd_t *pmd, un
 #endif
 		set_pte_at(&init_mm, addr, pte, pfn_pte(pfn, prot));
 		pfn++;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PFN_DOWN(size) * PTE_STEP, addr += size, addr != end);
+#else
 	} while (pte += PFN_DOWN(size), addr += size, addr != end);
+#endif
 	*mask |= PGTBL_PTE_MODIFIED;
 	return 0;
 }
@@ -331,7 +335,11 @@ static void vunmap_pte_range(pmd_t *pmd,
 	do {
 		pte_t ptent = ptep_get_and_clear(&init_mm, addr, pte);
 		WARN_ON(!pte_none(ptent) && !pte_present(ptent));
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	*mask |= PGTBL_PTE_MODIFIED;
 }
 
@@ -466,7 +474,6 @@ static int vmap_pages_pte_range(pmd_t *p
 	 * nr is a running index into the array which helps higher level
 	 * callers keep track of where we're up to.
 	 */
-
 	pte = pte_alloc_kernel_track(pmd, addr, mask);
 	if (!pte)
 		return -ENOMEM;
@@ -479,7 +486,11 @@ static int vmap_pages_pte_range(pmd_t *p
 			return -ENOMEM;
 		set_pte_at(&init_mm, addr, pte, mk_pte(page, prot));
 		(*nr)++;
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	*mask |= PGTBL_PTE_MODIFIED;
 	return 0;
 }
