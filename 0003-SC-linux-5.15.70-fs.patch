diff -rup linux-5.15.70-orig/fs/ext2/ext2.h linux-5.15.70/fs/ext2/ext2.h
--- linux-5.15.70-orig/fs/ext2/ext2.h	2022-09-23 22:15:52.000000000 +1000
+++ linux-5.15.70/fs/ext2/ext2.h	2022-10-04 19:26:32.879489252 +1100
@@ -177,7 +177,11 @@ static inline struct ext2_sb_info *EXT2_
  * Macro-instructions used to manage several block sizes
  */
 #define EXT2_MIN_BLOCK_SIZE		1024
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+#define	EXT2_MAX_BLOCK_SIZE		PAGE_SIZE
+#else
 #define	EXT2_MAX_BLOCK_SIZE		4096
+#endif
 #define EXT2_MIN_BLOCK_LOG_SIZE		  10
 #define EXT2_BLOCK_SIZE(s)		((s)->s_blocksize)
 #define	EXT2_ADDR_PER_BLOCK(s)		(EXT2_BLOCK_SIZE(s) / sizeof (__u32))
diff -rup linux-5.15.70-orig/fs/ntfs3/fslog.c linux-5.15.70/fs/ntfs3/fslog.c
--- linux-5.15.70-orig/fs/ntfs3/fslog.c	2022-09-23 22:15:52.000000000 +1000
+++ linux-5.15.70/fs/ntfs3/fslog.c	2022-10-04 19:26:32.883489286 +1100
@@ -1180,8 +1180,9 @@ static int log_read_rst(struct ntfs_log
 			struct restart_info *info)
 {
 	u32 skip, vbo;
-	struct RESTART_HDR *r_page = kmalloc(DefaultLogPageSize, GFP_NOFS);
-
+        struct RESTART_HDR *r_page = kmalloc(PAGE_SIZE > DefaultLogPageSize ?
+						 PAGE_SIZE : DefaultLogPageSize,		
+						 GFP_NOFS);
 	if (!r_page)
 		return -ENOMEM;
 
diff -rup linux-5.15.70-orig/fs/proc/task_mmu.c linux-5.15.70/fs/proc/task_mmu.c
--- linux-5.15.70-orig/fs/proc/task_mmu.c	2022-09-23 22:15:52.000000000 +1000
+++ linux-5.15.70/fs/proc/task_mmu.c	2022-10-04 19:26:32.883489286 +1100
@@ -609,7 +609,11 @@ static int smaps_pte_range(pmd_t *pmd, u
 	 * in here.
 	 */
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr != end; pte += PTE_STEP, addr += PAGE_SIZE)
+#else
 	for (; addr != end; pte++, addr += PAGE_SIZE)
+#endif
 		smaps_pte_entry(pte, addr, walk);
 	pte_unmap_unlock(pte - 1, ptl);
 out:
@@ -1165,7 +1169,11 @@ out:
 		return 0;
 
 	pte = pte_offset_map_lock(vma->vm_mm, pmd, addr, &ptl);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr != end; pte += PTE_STEP, addr += PAGE_SIZE) {
+#else
 	for (; addr != end; pte++, addr += PAGE_SIZE) {
+#endif
 		ptent = *pte;
 
 		if (cp->type == CLEAR_REFS_SOFT_DIRTY) {
@@ -1502,7 +1510,11 @@ static int pagemap_pmd_range(pmd_t *pmdp
 	 * goes beyond vma->vm_end.
 	 */
 	orig_pte = pte = pte_offset_map_lock(walk->mm, pmdp, addr, &ptl);
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	for (; addr < end; pte += PTE_STEP, addr += PAGE_SIZE) {
+#else
 	for (; addr < end; pte++, addr += PAGE_SIZE) {
+#endif
 		pagemap_entry_t pme;
 
 		pme = pte_to_pagemap_entry(pm, vma, addr, *pte);
@@ -1841,8 +1853,11 @@ static int gather_pte_stats(pmd_t *pmd,
 		if (!page)
 			continue;
 		gather_stats(page, md, pte_dirty(*pte), 1);
-
+#ifdef CONFIG_ARM_64KB_MMU_PAGE_SIZE_SUPPORT
+	} while (pte += PTE_STEP, addr += PAGE_SIZE, addr != end);
+#else
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+#endif
 	pte_unmap_unlock(orig_pte, ptl);
 	cond_resched();
 	return 0;
